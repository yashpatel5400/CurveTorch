{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed2d053",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yashpatel5400/CurveTorch/blob/main/tutorials/basic_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe38fd",
   "metadata": {
    "id": "defe38fd"
   },
   "source": [
    "## Overview\n",
    "CurveTorch can be used interchangeably with any other PyTorch optimizer. The only thing that may be different from most optimizers you usually use is the need to pass in the optimizer function closure. This allows the optimizer to access Hessian information during the update step. We will see an example of how to do this further below. Let's start by importing the usual packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e047b",
   "metadata": {
    "id": "b70e047b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d2487",
   "metadata": {
    "id": "173d2487"
   },
   "source": [
    "Let's now continue by importing the CurveSGD package. If you have installed it globally, there is no need to add the `sys.append`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937b8d3",
   "metadata": {
    "id": "3937b8d3"
   },
   "outputs": [],
   "source": [
    "# hack for importing local library: not necessary if installed globally\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import curvetorch as curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd73a3",
   "metadata": {
    "id": "62cd73a3"
   },
   "source": [
    "## CurveSGD Usage\n",
    "To use CurveSGD, we need to define a model that we wish to optimize. CurveSGD can also be used in isolation for optimizing functions outside the scope of neural networks, so long as the functions are defined using the PyTorch API and have autograd available. See the other tutorial if you are interested in using CurveSGD for purposes beyond optimizing neural networks. Here, we start by define a very simple network, which will be run on MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f80fc4",
   "metadata": {
    "id": "c6f80fc4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(784, 4) \n",
    "        self.fc2 = nn.Linear(4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0069329",
   "metadata": {
    "id": "b0069329"
   },
   "source": [
    "Finally, let's see how to actually use CurveSGD in an optimization loop (note: the \"optimizer\" referenced in the code below is an instance of CurveSGD, passed in further below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa177ba1",
   "metadata": {
    "id": "fa177ba1"
   },
   "outputs": [],
   "source": [
    "def train(conf, model, device, train_loader, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward(retain_graph=True, create_graph=True)\n",
    "            return loss\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        loss = F.nll_loss(model(data), target)\n",
    "        if batch_idx % conf.log_interval == 0:\n",
    "            loss = loss.item()\n",
    "            idx = batch_idx + epoch * (len(train_loader))\n",
    "            writer.add_scalar('Loss/train', loss, idx)\n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss,\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a4581",
   "metadata": {
    "id": "994a4581"
   },
   "source": [
    "As mentioned towards the beginning of the tutorial, we have a chunk of code that is somewhat atypical of standard optimizers: the use of function closures. Generally, optimizers perform updates via: `optimizer.step()`. In this case, however, we have:\n",
    "\n",
    "```\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward(retain_graph=True, create_graph=True)\n",
    "    return loss\n",
    "        \n",
    "optimizer.step(closure)\n",
    "```\n",
    "\n",
    "With the reason being that we need the entire closure of the loss to access Hessian information within the optimization step. Other optimizers that require 2nd order information similarly make use of function closures, but they are less common than 1st order methods, which can get away with simply being invoked using `optimizer.step()`. With this setup complete, we can at last go ahead and prep up some final accessory functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00450239",
   "metadata": {
    "id": "00450239"
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(conf, use_cuda=False):\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            '../data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=conf.batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            '../data',\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=conf.test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 64,\n",
    "        test_batch_size: int = 1000,\n",
    "        epochs: int = 15,\n",
    "        lr: float = 0.01,\n",
    "        gamma: float = 0.7,\n",
    "        no_cuda: bool = True,\n",
    "        seed: int = 42,\n",
    "        log_interval: int = 10,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.no_cuda = no_cuda\n",
    "        self.seed = seed\n",
    "        self.log_interval = log_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69437982",
   "metadata": {
    "id": "69437982"
   },
   "source": [
    "Finally, let's define the invocation of the optimization loop. Note the invocation:\n",
    "\n",
    "```\n",
    "optimizer = curve.CurveSGD(model.parameters(), lr=conf.lr)\n",
    "```\n",
    "\n",
    "There are some other parameters related to the exponential moving averages of the function, gradient, and Hessian-vector values, which can be seen in full detail in the accompanying full documentation. With that, let's define the optimization loop with the appropriate invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcab58d",
   "metadata": {
    "id": "3bcab58d"
   },
   "outputs": [],
   "source": [
    "def run_optimizer():\n",
    "    conf = Config()\n",
    "    log_dir = 'runs/mnist_custom_optim'\n",
    "    print('Tensorboard: tensorboard --logdir={}'.format(log_dir))\n",
    "\n",
    "    with SummaryWriter(log_dir) as writer:\n",
    "        use_cuda = not conf.no_cuda and torch.cuda.is_available()\n",
    "        torch.manual_seed(conf.seed)\n",
    "        device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        train_loader, test_loader = prepare_loaders(conf, use_cuda)\n",
    "\n",
    "        model = Net().to(device)\n",
    "\n",
    "        # create grid of images and write to tensorboard\n",
    "        images, labels = next(iter(train_loader))\n",
    "        img_grid = utils.make_grid(images)\n",
    "        writer.add_image('mnist_images', img_grid)\n",
    "\n",
    "        # custom optimizer from torch_optimizer package\n",
    "        optimizer = curve.CurveSGD(model.parameters(), lr=conf.lr)\n",
    "\n",
    "        for epoch in range(1, conf.epochs + 1):\n",
    "            train(conf, model, device, train_loader, optimizer, epoch, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3c85c",
   "metadata": {
    "id": "5ec3c85c"
   },
   "source": [
    "And at last, we can run the accompanying optimizer simply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0a439",
   "metadata": {
    "id": "51c0a439"
   },
   "outputs": [],
   "source": [
    "run_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545cd5b1",
   "metadata": {
    "id": "545cd5b1"
   },
   "source": [
    "And that's it! Notice that there is nothing different about the invocation of CurveSGD for optimization compared to other optimizers in the PyTorch library, with the exception of having to invoke the function closure for Hessian information. For full documentation, see the associated website or Sphinx pages."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "basic_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
